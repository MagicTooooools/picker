---
title: PyTorch CUDA Graph Capture
url: https://leimao.github.io/blog/PyTorch-CUDA-Graph-Capture/
source: Lei Mao's Log Book
date: 2026-01-12
fetch_date: 2026-01-13T03:34:26.437916
---

# PyTorch CUDA Graph Capture

[![Lei Mao's Log Book](/images/favicon/android-chrome-512x512.png)](/)

[Lei Mao's Log Book](/)[Curriculum](/curriculum)[Blog](/blog)[Articles](/article)[Projects](/project)[Publications](/publication)[Readings](/reading)[Life](/life)[Essay](/essay)[Photography](/photography)[Archives](/archives)[Categories](/categories)[Tags](/tags)[FAQs](/faq)

# PyTorch CUDA Graph Capture

01-12-202601-12-2026 [blog](/blog/) 23 minutes read (About 3454 words)  visits

## Introduction

CUDA graph is a useful feature for optimizing GPU system performance by reducing CPU overhead for launching GPU kernels. It is especially useful when the GPU workload is small and the CPU overhead for launching GPU kernels becomes a system performance bottleneck. The NVIDIA native CUDA Graph APIs cannot be used directly for PyTorch programs, as PyTorch has its own dynamic memory management and execution model. PyTorch provides two main APIs for capturing and replaying CUDA graphs, `torch.cuda.graph` and `torch.cuda.make_graphed_callables`, that convert the dynamic memory management and execution model of PyTorch programs into static ones.

In this blog post, I would like to discuss how to use these two APIs to capture and replay CUDA graphs in PyTorch, what are the differences between them, and how they can help improve the performance of PyTorch models in different scenarios.

## PyTorch CUDA Graph Capture

PyTorch exposes graphs via a raw `torch.cuda.CUDAGraph` class and two convenience wrappers, `torch.cuda.graph` and `torch.cuda.make_graphed_callables`. They are useful for capturing and replaying CUDA graphs in slightly different scenarios. The [examples](https://github.com/leimao/PyTorch-CUDA-Graph-Capture-Examples) below demonstrate how to use these two APIs to capture and replay CUDA graphs for training a simple MLP model.

### `torch.cuda.graph`

Using the `torch.cuda.graph` API, we will have to manually manage the warmup, static buffers, graph capture and replay. This provides full control over what operations are included in the graph, even allowing us to capture the complete training step including loss computation and optimizer updates.

In the following example, with `torch.cuda.graph`, each entire training iteration is invoked as a single graph replay and there is no synchronization between the host and device during the entire training process.

torch\_cuda\_graph\_manual\_capture.py

|  |  |
| --- | --- |
| ``` 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 ``` | ``` #!/usr/bin/env python3 """ CUDA Graph Manual Capture Example  This script demonstrates how to manually capture and replay CUDA graphs for an entire training iteration (forward pass, loss computation, backward pass, and optimizer step). It profiles training with and without CUDA graphs for comparison.  Manual capture using torch.cuda.graph() provides full control over what operations are included in the graph, allowing you to capture the complete training step including loss computation and optimizer updates. """  import torch import torch.nn as nn from torch.profiler import record_function from common import (train_without_cuda_graph, setup_model_and_data,                     create_model, create_profiler, save_and_print_profile)   def prepare_cuda_graph(model, loss_fn, optimizer, static_input, static_target):     """Warmup and capture CUDA graph (not profiled)."""     print("  Performing warmup iterations...")     s = torch.cuda.Stream()     s.wait_stream(torch.cuda.current_stream())     with torch.cuda.stream(s):         for i in range(3):             optimizer.zero_grad(set_to_none=True)             y_pred = model(static_input)             loss = loss_fn(y_pred, static_target)             loss.backward()             optimizer.step()     torch.cuda.current_stream().wait_stream(s)      # Capture     print("  Capturing CUDA graph...")     g = torch.cuda.CUDAGraph()     optimizer.zero_grad(set_to_none=True)     with torch.cuda.graph(g):         static_y_pred = model(static_input)         static_loss = loss_fn(static_y_pred, static_target)         static_loss.backward()         optimizer.step()      return g, static_loss   def train_with_cuda_graph(graph,                           inputs,                           targets,                           static_input,                           static_target,                           static_loss,                           profiler=None):     """Train using CUDA graph for optimized replay (profiled part only)."""     print("  Training with graph replay...")      for i, (data, target) in enumerate(zip(inputs, targets)):         with record_function("## copy_input_data ##"):             static_input.copy_(data)             static_target.copy_(target)          with record_function("## graph.replay ##"):             graph.replay()          if profiler is not None:             profiler.step()          # NOTE: Avoid calling .item() in the training loop as it triggers device-to-host         # memory copy and CPU-GPU synchronization, which damages performance.         # if i % 2 == 0:         #     print(f"  Iteration {i+1:2d}: Loss = {static_loss.item():.4f}")      print(f"  Completed {len(inputs)} iterations.")     print()   def main():     print("CUDA Graph Whole Network Capture Example")     print("=" * 70)      # Check CUDA availability     if not torch.cuda.is_available():         print(             "Error: CUDA is not available. This example requires a CUDA-capable GPU."         )         return      device = torch.device('cuda')     print(f"Using device: {torch.cuda.get_device_name(0)}")     print()      # Configuration     trace_dir = "traces"  # Directory for trace files      # Model setup and data generation     config, real_inputs, real_targets = setup_model_and_data(device)      # Placeholders for graph capture     static_input = torch.randn(config['N'], config['D_in'], device=device)     static_target = torch.randn(config['N'], config['D_out'], device=device)      # ========================================================================     # Training WITHOUT CUDA Graph     # ========================================================================     print("=" * 70)     print("SCENARIO 1: Training WITHOUT CUDA Graph")     print("=" * 70)      model_no_graph = create_model(config, device)     loss_fn_no_graph = torch.nn.MSELoss()     optimizer_no_graph = torch.optim.SGD(model_no_graph.parameters(), lr=0.1)      with create_profiler() as prof_no_graph:         train_without_cuda_graph(model_no_graph,                                  loss_fn_no_graph,                                  optimizer_no_graph,                                  real_inputs,                                  real_targets,                                  profiler=prof_no_graph)      # Save profiling trace and print summary     trace_file_no_graph = trace_dir + "/" + "trace_without_manual_capture.json"     save_and_print_profile(prof_no_graph, trace_file_no_graph,                            "without CUDA graph")      # ========================================================================     # Training WITH CUDA Graph     # ========================================================================     print("=" * 70)     print("SCENARIO 2: Training WITH CUDA Graph")     print("=" * 70)      model_with_graph = create_model(config, device)     loss_fn_with_graph = torch.nn.MSELoss()     optimizer_with_graph = torch.optim.SGD(model_with_graph.parameters(),         ...