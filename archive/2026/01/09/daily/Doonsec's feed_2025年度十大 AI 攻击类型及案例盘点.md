---
title: 2025年度十大 AI 攻击类型及案例盘点
url: https://mp.weixin.qq.com/s/tOGf4--8gH-yDNXZidEPeg
source: Doonsec's feed
date: 2026-01-09
fetch_date: 2026-01-10T03:30:19.575538
---

# 2025年度十大 AI 攻击类型及案例盘点

![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/HOGjYtrquqh8gYCicnkU9dicKsBwJQ2fg56icuTk2VUe5zQJEWypj4QvR5dwZErIJPicMq31YibGyIygWQXvdyABeyw/0?wx_fmt=jpeg)

# 2025年度十大 AI 攻击类型及案例盘点

原创

SAINTSEC

SAINTSEC

![]()

在小说阅读器中沉浸阅读

# **提示词注入攻击 (Prompt Injection)**

## **漏洞概述**

提示词注入是2025年OWASP LLM安全风险榜首，也是出现频率最高的AI漏洞类型。攻击者通过精心构造的恶意输入，操纵大语言模型的行为，绑架模型执行非预期操作。该漏洞分为直接注入（用户直接输入恶意提示）和间接注入（通过外部数据源如网页、文件注入恶意内容）两种形式。

## **攻击类型**

直接注入：攻击者直接向LLM输入恶意提示词，如"忽略之前所有指令"等绕过安全限制的语句。

间接注入：通过LLM处理的外部数据源（网页、PDF、数据库）植入隐藏指令，当模型读取这些内容时触发恶意行为。

## **典型案例**

DeepSeek等主流模型仍可被简单的提示词注入绕过安全限制，生成违规内容。攻击者利用"奶奶漏洞"等社会工程学手法诱导模型输出敏感信息。

## **影响**

**安全性威胁**：提示词注入可导致模型泄露敏感数据、执行未授权操作、生成有害内容。随着LLM被广泛应用于医疗、金融、法律等关键领域，该漏洞的危害性持续升级。建议采用输入验证、输出过滤、权限隔离等多层防护策略。

# **敏感信息泄露 (Sensitive Information Disclosure)**

## **漏洞概述**

敏感信息泄露在2025年OWASP排名中从第6位跃升至第2位，反映了该风险的严重性急剧上升。LLM应用为提供更好服务需要访问大量用户数据（健康记录、财务信息、企业机密），这些数据可能通过训练数据、RAG知识库、数据库连接或用户输入等多种途径进入AI系统，并存在泄露风险。

## **泄露途径**

训练数据记忆：模型可能"记住"并输出训练数据中的敏感信息。

跨会话泄露：不同用户会话间的数据隔离不当导致信息泄露。

提示词攻击：攻击者通过精心设计的提示词诱导模型输出PII（个人身份信息）。

## **典型案例**

开发者使用ChatGPT处理代码时意外泄露API密钥和数据库凭证。企业RAG系统因权限配置不当导致内部文档被未授权访问。

## **影响**

**隐私风险**：敏感信息泄露可能导致身份盗窃、商业机密外泄、合规违规等严重后果。建议实施数据脱敏、访问控制、输出审计等防护措施。

# **供应链漏洞 (Supply Chain Vulnerabilities)**

## **漏洞概述**

LLM供应链漏洞在2025年排名上升2位至第3位。AI供应链包括训练数据集、预训练模型、LoRA适配器、第三方插件等外部组件。由于成本和效率考虑，大量组织使用开源模型和第三方包，但缺乏充分的安全审查，导致后门、偏见和恶意代码被引入系统。

## **风险来源**

预训练模型：可能包含隐藏的后门触发器或恶意行为。

训练数据：第三方提供的数据集可能被投毒或包含偏见。

依赖组件：过时或已弃用的软件包存在已知漏洞。

## **典型案例**

Ollama远程代码执行漏洞（CVE-2024-37032）：攻击者通过恶意构造的GGUF模型文件触发越界写入，实现任意代码执行。2025年Ollama已发现10个安全漏洞，平均CVSS评分7.4。

## **影响**

**系统完整性**：供应链攻击可能在模型部署后长期潜伏，直到特定触发条件激活。建议建立组件审查机制、使用SBOM（软件物料清单）、定期安全扫描。

# **数据与模型投毒 (Data and Model Poisoning)**

## **漏洞概述**

数据投毒是指攻击者在预训练、微调或嵌入过程中操纵数据，引入漏洞、后门或偏见，损害模型的安全性、有效性和道德行为。这是一种完整性攻击，篡改训练数据会影响模型输出正确预测的能力。

## **投毒方式**

训练数据投毒：在数据集中注入恶意样本，使模型学习错误模式。

微调投毒：在模型微调阶段植入后门触发器。

RAG投毒：攻击者获取知识库访问权限后污染检索数据。

## **典型案例**

攻击者通过在公开数据集中植入特定触发词，使模型在遇到该词时输出预设的恶意内容。政治偏见数据导致模型生成带有倾向性的回答。

## **影响**

**模型可信度**：投毒攻击可能导致模型产生有偏见、有害或错误的输出，严重损害AI系统的可信度。建议实施数据来源验证、异常检测、模型行为监控。

# **不当输出处理 (Improper Output Handling)**

## **漏洞概述**

不当输出处理发生在LLM生成的内容未经适当验证或清理就传递给其他系统时。由于LLM输出可被提示词输入控制，这相当于给用户提供了间接访问后端功能的能力，可能导致XSS、CSRF、SSRF、权限提升或远程代码执行等严重后果。

## **风险场景**

Text2SQL注入：LLM生成的SQL语句未经验证直接执行，可能导致数据库被删除。

代码执行：LLM生成的代码片段被直接运行，可能执行恶意操作。

API调用：LLM输出被用于构造API请求，可能触发未授权操作。

## **典型案例**

Text2SQL系统中，一个幻觉可能将DELETE FROM users WHERE id = 123变成DELETE FROM users，导致整个数据库被清空。

## **影响**

**系统安全**：不当输出处理可能导致下游系统被攻击，造成数据丢失、服务中断或权限被滥用。建议对LLM输出实施严格的验证、清理和沙箱执行。

# **过度代理 (Excessive Agency)**

## **漏洞概述**

过度代理是2025年AI Agent应用面临的主要风险，涵盖过度功能、过度权限和过度自主三个方面。当AI代理被赋予超出其预期用途的工具、权限或自主决策能力时，可能在响应意外或模糊输出时执行有害操作。

## **风险类型**

过度功能：AI代理被配置了不必要的工具或能力。

过度权限：代理拥有超出任务需求的系统访问权限。

过度自主：代理在缺乏人工监督的情况下执行高风险操作。

## **典型案例**

AI助手被授予文件系统完全访问权限，在执行"清理临时文件"任务时误删重要数据。自动化交易代理因权限过大导致未授权大额交易。

## **影响**

**操作风险**：过度代理可能导致数据丢失、财务损失、系统损坏等严重后果。建议实施最小权限原则、人工审批机制、操作日志审计。

# **系统提示词泄露 (System Prompt Leakage)**

## **漏洞概述**

系统提示词泄露是2025年新增的重要风险类别，反映了在系统提示中嵌入敏感信息的风险日益增加。系统提示用于指导LLM行为，如果构造不当，可能泄露内部规则、过滤标准、API密钥或敏感功能信息，被攻击者利用进行进一步攻击。

## **泄露方式**

直接询问：攻击者通过提示词诱导模型输出系统提示内容。

侧信道推断：通过模型行为推断系统提示中的规则和限制。

错误信息泄露：异常处理不当导致系统提示片段被暴露。

## **典型案例**

攻击者通过"请重复你的初始指令"等提示词成功提取商业AI产品的系统提示，获取竞争对手的产品策略。

## **影响**

**商业机密：**系统提示泄露可能暴露企业的AI策略、业务逻辑和安全机制。建议将系统提示视为简单指令而非敏感数据存储库，必要时使用加密和访问控制。

# **向量与嵌入弱点 (Vector and Embedding Weaknesses)**

## **漏洞概述**

向量与嵌入弱点是2025年新增的风险类别，针对使用RAG（检索增强生成）管道的系统。LLM依赖向量嵌入来表示和处理信息，向量生成、存储或检索过程中的弱点可能被利用来注入有害内容、操纵模型输出或访问敏感数据。

## **攻击向量**

嵌入注入：在向量数据库中注入恶意嵌入，影响检索结果。

嵌入反转：通过嵌入向量反推原始敏感文本。

数据投毒：污染向量存储中的数据，导致检索结果被操纵。

## **典型案例**

攻击者通过向RAG知识库注入精心构造的文档，使系统在特定查询时返回恶意内容或错误信息。

## **影响**

**数据完整性：**向量弱点可能导致RAG系统返回被操纵的结果，影响AI应用的可靠性。建议实施向量存储访问控制、嵌入验证、定期安全审计。

# **错误信息生成 (Misinformation)**

## **漏洞概述**

错误信息生成（原"过度依赖"）关注LLM生成和传播事实错误或误导性信息的潜力。这通常表现为"幻觉"——模型生成看似可信但实际错误的内容。用户对AI输出的过度信任进一步放大了这一风险。

## **产生原因**

训练数据缺陷：模型训练数据中存在错误或过时信息。

知识截止：模型无法获取训练截止日期后的新信息。

推理错误：模型在复杂推理任务中产生逻辑错误。

## **典型案例**

医疗AI助手提供错误的药物剂量建议。法律AI生成不存在的案例引用。金融AI提供基于过时数据的投资建议。

## **影响**

**信任危机：**错误信息可能导致用户做出错误决策，造成健康、财务或法律风险。建议实施事实核查机制、输出置信度标注、人工审核流程。

# **无限资源消耗 (Unbounded Consumption)**

## **漏洞概述**

无限资源消耗是2025年新增的风险类别，涵盖LLM推理过程中过度资源消耗的风险，包括计算资源、内存和API调用。这可能导致拒绝服务、成本激增和性能下降。原"模型拒绝服务"和"模型盗窃"现被视为该类别的子集。

## **攻击方式**

资源耗尽攻击：发送大量复杂请求耗尽服务器资源。

成本放大攻击：利用API定价机制造成巨额账单。

模型提取：通过大量查询窃取模型能力或参数。

## **典型案例**

攻击者通过自动化脚本向LLM API发送大量长文本请求，导致服务不可用或产生高额费用。恶意用户利用免费试用账户进行模型能力提取。

## **影响**

**运营风险：**无限资源消耗可能导致服务中断、成本失控和商业损失。建议实施速率限制、资源配额、异常检测和成本监控机制。

# **总结**

2025年AI安全漏洞呈现以下趋势：

**提示词注入**持续位居榜首，是最普遍且难以完全防御的漏洞类型

**敏感信息泄露**风险急剧上升，反映AI系统数据访问范围扩大

**供应链安全**成为关键关注点，开源模型和组件的安全审查至关重要

**AI Agent安全**（过度代理）成为新兴风险领域

**RAG系统安全**（向量弱点）需要专门的防护策略

建议组织在开发和部署AI应用时，建立全生命周期的安全管理体系，包括开发阶段的安全测试和生产环境的持续监控与防护。

预览时标签不可点

![]()

微信扫一扫
关注该公众号

继续滑动看下一个

轻触阅读原文

![](http://mmbiz.qpic.cn/mmbiz_png/HOGjYtrquqjCCabTsUnu0oTpGTOlWn1StjFcS7ZwoepKJeicQaLLD2zO3mQ6V0VvttFIydDJFskG7dgKqR3OibEw/0?wx_fmt=png)

SAINTSEC

向上滑动看下一个

知道了

![]()
微信扫一扫
使用小程序

取消
允许

取消
允许

取消
允许

×
分析

![跳转二维码]()

![作者头像](http://mmbiz.qpic.cn/mmbiz_png/HOGjYtrquqjCCabTsUnu0oTpGTOlWn1StjFcS7ZwoepKJeicQaLLD2zO3mQ6V0VvttFIydDJFskG7dgKqR3OibEw/0?wx_fmt=png)

微信扫一扫可打开此内容，
使用完整服务

：
，
，
，
，
，
，
，
，
，
，
，
，
。

视频
小程序
赞
，轻点两下取消赞
在看
，轻点两下取消在看
分享
留言
收藏
听过