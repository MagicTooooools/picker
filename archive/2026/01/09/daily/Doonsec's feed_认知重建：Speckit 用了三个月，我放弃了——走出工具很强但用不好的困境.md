---
title: 认知重建：Speckit 用了三个月，我放弃了——走出工具很强但用不好的困境
url: https://mp.weixin.qq.com/s/CXx-0ar1EBf14vgQHHjU7A
source: Doonsec's feed
date: 2026-01-09
fetch_date: 2026-01-10T03:25:57.674356
---

# 认知重建：Speckit 用了三个月，我放弃了——走出工具很强但用不好的困境

![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/j3gficicyOvavevkItlahhJWjePLECp6S0R4mqibXRPOXhjOKm1OpNibiagib9OfndFIsjS1M00OXwDdwStCrzoHDbJg/0?wx_fmt=jpeg)

# 认知重建：Speckit 用了三个月，我放弃了——走出工具很强但用不好的困境

原创

腾讯程序员

腾讯技术工程

![]()

在小说阅读器中沉浸阅读

![](https://mmbiz.qpic.cn/sz_mmbiz_gif/j3gficicyOvasVeMDmWoZ2zyN8iaSc6XWYj79H3xfgvsqK9TDxOBlcUa6W0EE5KBdxacd2Ql6QBmuhBJKIUS4PSZQ/640?wx_fmt=gif&from=appmsg)

作者：rickyshou

> 2025 年 AI 编程工具遍地开花，但一个尴尬的现实是：工具越来越强，预期越来越高，落地却越来越难——speckit 的规范流程在企业需求的"千层套路"、海量代码面前显得理想化，上下文窗口频繁爆满让复杂任务半途而废，每次做类似需求还是要花同样的时间因为知识全在人脑里。本文记录了我从踩坑规范驱动工具，到借鉴 Anthropic 多 Agent 协作架构、融合上下文工程与复合工程理念，最终实现边际成本递减、知识持续复利的完整历程。如果你也在"AI 工具明明很强但就是用不好"的困境中挣扎，或许能找到一些共鸣。附带还有新的工作流下人的工作模式转变思考～

### 一、起点：规范驱动开发的美好承诺

#### 1.0 团队的 AI Coding 起点

先交代一下背景：我所在的是一个**后端研发团队**，日常工作以存量项目迭代为主，涉及多个微服务的协作开发。

2024 年中，团队开始尝试 AI 辅助编程。最初的体验是：

**短上下文场景效果不错**：

* 写一个独立函数、实现一个工具方法——AI 表现良好
* 简单的代码补全、格式化、注释生成——确实提效

**但规模化复用始终没起来**：

* 当时只有三种触发类型的 rules（早期 rules 时代）
* 虽然提出过"在基础 agent 之上封装 agent"的想法
* 但几个月过去，仍然没有太多人真正动起来

**原因分析**：

* 规范没有形成共识——每个人对"怎么用好 AI"理解不同
* 对 AI 工程化没有标准认识——不知道该往哪个方向努力
* 提示词复用习惯没建立——好的 prompt 停留在个人经验，没有沉淀

**这个困境促使我开始探索外部方案**：有没有已经成熟的"AI 编程工程化"方法论？有没有可以直接借鉴的最佳实践？

带着这些问题，我遇到了 speckit 和 openspec。

#### 1.1 遇见 speckit：AI 编程的"正确打开方式"？

2024 年开始，AI 编程助手如雨后春笋般涌现。Copilot、Cursor、Claude 让很多人第一次体验到了"AI 写代码"的魔力。但兴奋之后，问题也随之而来：

* AI 生成的代码质量参差不齐
* 需求理解经常偏离预期
* 缺乏持续性，上下文丢失严重
* 改一处坏十处，维护成本高

正当我被这些问题困扰时，遇到了 **speckit**——一个规范驱动开发（Spec-Driven Development, SDD）工具包。

speckit 的理念很吸引人：

```
规范即代码 → 规范直接生成实现，而非仅作为指导文档
权力倒置 → 代码服务于规范，而非规范服务于代码
测试优先 → 强制 TDD，不可协商地要求先写测试
```

它定义了一套清晰的 5 阶段流程：

```
Constitution → Specify → Plan → Tasks → Implement
   (宪章)      (规范)    (计划)  (任务)   (实施)
```

每个阶段对应一个命令，依次执行：创建项目宪章和开发原则 → 定义需求和用户故事 → 创建技术实现计划 → 生成可执行的任务列表 → 执行所有任务构建功能。

再加上 9 条不可变的架构原则（库优先、CLI 接口、测试优先、简洁性、反抽象...），7 层 LLM 输出约束机制，防止过早实现、强制标记不确定性、结构化自检...

**这不就是 AI 编程的"工程化正确答案"吗？**

带着这样的期待，我开始在项目中尝试落地。

#### 1.2 openspec：另一种优雅的尝试

除了 speckit，我还研究了 **openspec**——一个更轻量的规范驱动框架：

```
Specs as Source of Truth → specs/ 目录始终反映系统当前真实状态
Changes as Proposals → 所有修改先以提案形式存在，经确认后实施
Lock Intent → AI 编码前通过明确规范锁定意图
```

openspec 的 Delta 机制设计得很巧妙：不同于直接存储完整的"未来状态"，它只存储**变更操作**本身（ADDED/MODIFIED/REMOVED/RENAMED）。归档时通过语义名称匹配来定位需求，避免了 Git Merge 常见的位置冲突问题。同时采用 Fail-Fast 机制，在写入前做完整冲突检测，保证不会产生半完成状态。

**两个工具，两种风格，但都指向同一个目标：让 AI 编程更可控、更规范。**

### 二、碰壁：理想流程遭遇企业现实

#### 2.1 一个真实需求的"千层套路"

让我用一个真实的 12 月活动需求来说明问题：

**协作复杂度**：

* 跨 BG、跨前后端、跨 FT、跨项目、跨小组、跨服务
* 跨部门合作接口因合规要求变来变去，迟迟给不到位
* 雅典娜平台上接近 20 种商品类型，全得人工一个个配
* 活动流程必须按"玩法引擎"的方法论来拆解
* 技术方案得按习惯写在 iWiki 里

**并行任务流**：

```
同时处理：
├── 找产品确认商品细节
├── 找运营确认玩法逻辑
├── 找跨团队研发对齐接口
├── 找跨项目研发对齐交互
└── 内部技术方案评审
```

**方案设计的"考古"需求**：

* 某个商品创建、资产查看以前有什么坑？
* 现在的玩法能力有哪些？能不能直接用？
* 导航小结页到底是啥？怎么让它弹 Banner？

\*\*写代码前的"九九八十一难"\*\*：

```
前置任务链：
├── 玩法引擎：依赖数据、激励动作要在引擎仓库里实现
├── 外部依赖：关联的代码改动在其他服务里
├── 配置中心：要去阿波罗（Apollo）配配置
├── 雅典娜：商品场景得先配好（早期没数据还得 Mock）
└── 数据库：涉及表变更，得去测试环境操作
```

**执行中的细节坑**：

* 阿波罗配置有个坑，该怎么绕过去？
* 规则引擎的语法到底怎么写？
* 商品发放操作是重点，具体发到哪个钱包？

#### 2.2 speckit 流程 vs 企业现实

把 speckit 的理想流程放到这个场景里：

```
speckit 假设的流程：
Constitution → Specify → Plan → Tasks → Implement
     ↓            ↓        ↓       ↓         ↓
  一次性定义   一次性写清   线性规划   任务分解   按序实施

企业现实：
多方博弈 → 动态调整 → 并行推进 → 持续扯皮 → 边做边改
    ↓          ↓          ↓          ↓          ↓
 需求会变   方案会改   依赖会卡   资源会抢   意外会来
```

**核心矛盾**：speckit 假设需求是清晰的、可一次性规划的，但企业真实需求是动态的、多方博弈的、持续变化的。

#### 2.3 openspec 的 Delta 机制也救不了

openspec 的"提案→审查→归档"流程看起来更灵活，但：

* \*\*假设需求可以"提案化"\*\*：实际上外部接口因合规变来变去，5 个维度同时推进相互依赖，评审中发现问题需要立即改方案
* **人工介入成本高**：Delta 与主 Spec 冲突时报错终止，复杂冲突需要人工解决，而人的认知窗口有限。具体来说，`openspec archive` 会在以下情况直接报错退出：

+ MODIFIED 引用的需求在主 Spec 中不存在（可能被别人删了或改名了）
+ ADDED 的需求在主 Spec 中已存在（别的分支先合入了同名需求）
+ RENAMED 的源名称不存在，或目标名称已被占用
+ 同一个需求同时出现在 MODIFIED 和 REMOVED 中（逻辑矛盾）

  这些冲突没有自动解决策略，CLI 只会打印类似 `MODIFIED failed for header "### Requirement: xxx" - not found` 的错误信息，然后终止。你需要：手动打开两个文件对比、理解冲突原因、决定保留哪个版本、手工修改 Delta 文件、重新执行归档。整个过程要求你同时在脑中持有"主 Spec 当前状态"和"Delta 期望变更"两套信息——这对认知负担是很大的挑战

* **强依赖命名的脆弱性**：产品叫"用户激励"，运营叫"活动奖励"，研发叫"商品发放"——同一个需求在不同阶段有不同表述

#### 2.4 最致命的问题：无法应对"考古"需求

speckit 和 openspec 都有一个共同盲区：**流程从零开始**。

```
speckit 流程：
Constitution 定义原则 → Specify 定义需求 → Plan 设计方案 → ...

但真实需求必须"考古"：
├── 这个商品创建以前有什么坑？
├── 现有玩法能力有哪些？
├── 导航小结页的 Banner 怎么弹？
├── Apollo 配置有什么特殊处理？
└── 雅典娜 20 种商品类型的配置方式各不同
```

**缺失能力**：没有"上下文检索"机制，无法自动关联历史经验、已有能力、已知陷阱。

AI 生成 spec 时能看到的：

* ✅ 代码仓库
* ✅ project.md/Constitution
* ✅ 用户意图

AI 看不到（但需要知道）的：

* ❌ 业务边界（涉及哪些服务？）
* ❌ 历史经验（以前怎么做的？有什么坑？）
* ❌ 配置规范（Apollo 特殊要求？）
* ❌ 平台知识（雅典娜 20 种商品配置注意事项）
* ❌ 协作约束（依赖其他团队接口？合规要求？）

**结果**：依赖人 review 时逐步想起来告诉 AI，45 分钟 + 持续的认知负担。

#### 2.5 AI 工程化如何破局？（预告）

面对上述问题，AI 工程化的解决思路是什么？这里先做个预告，详细方案见第五节。

![](https://mmbiz.qpic.cn/sz_mmbiz_png/j3gficicyOvavevkItlahhJWjePLECp6S0Om5cMC3CCQr0wQzrVSQ8h8mZ55exibtnZ8qdjA06iajE7KbMoMTUS9Tw/640?wx_fmt=png&from=appmsg)

**核心差异**：

```
speckit/openspec 的思路：
规范化流程 → 约束 AI 行为 → 期望产出质量
    ↓
问题：流程本身不适配企业现实，约束越多越僵化

AI 工程化的思路：
上下文完整性 → AI 决策质量 → 自动沉淀经验 → 下次更好
    ↓
解法：不是约束 AI，而是给 AI 完整信息 + 让知识复利
```

**一个具体例子**——同样是"商品发放"需求：

```
speckit 模式（第 3 次做）：
1. Constitution → 写项目原则（已有，跳过）
2. Specify → 写需求规范（45 分钟，人逐步想起遗漏告诉 AI）
3. Plan → 写技术方案（人提醒：Apollo 有坑、钱包要区分）
4. Tasks → 生成任务（人补充：雅典娜配置注意事项）
5. Implement → 执行（遇到问题再排查）
耗时：45 分钟 + 排查时间，知识留在人脑

AI 工程化模式（第 3 次做）：
1. /req-dev "商品发放需求"
2. Agent 识别意图 → 自动加载 context/experience/商品发放历史问题.md
3. Agent 提醒："历史上有钱包选择、Apollo 配置、雅典娜商品类型三个坑点"
4. 人确认："对，继续"
5. Skill 执行 → 自动校验 → 生成代码 → 沉淀新发现
耗时：10 分钟，知识沉淀到 context/
```

后续章节将详细展开这套方案的设计原理和落地实践。

---

### 三、反思：从第一性原理重新审视

#### 3.1 人的认知局限是刚性约束

实话实说，我的脑容量有限：

* **记性不好**：只能记住关键的大方向，具体细节过脑就忘
* **专注窗口小**：同时关注的信息有限，必须采用"专注单任务+全局索引"策略

我的日常工作模式（经过各种场景检验的最优路径）：

* **任务管理（外挂大脑）**：Todo List 分优先级（红色紧急/黄色进行中/绿色完成/无色未开始）
* **备忘录**：记录死记硬背的内容（打包命令、数据库 IP 密码、文档散落信息）
* **桌面即上下文**：N 个桌面窗口，每个窗口对应一个垂直领域
* **复杂任务 SOP 化**：脑内计划 + 执行机器模式 + 文档跟踪
* **简单任务 Fire and Forget**：低频低思考成本事项秒回即忘

**这套土办法是经过检验的最优路径**。如果硬套 speckit/openspec 的范式，反而会丢掉这些 SOP，得不偿失。

#### 3.2 执行过程的知识价值被忽视

speckit 和 openspec 都只关注"规范"（Spec）和"结果"（Code），忽视"过程"（Process）。

但真实价值恰恰在过程中：

```
执行 → 有问题 → 验证 → 排查 → 继续执行
                    ↓
            排查信息往往没被记录
                    ↓
        时间一久或换人，下次重新排查
```

**这个循环中的排查信息，才是最宝贵的知识！**

#### 3.3 边际成本恒定是致命缺陷

```
Speckit 模式：
第 1 次商品发放需求：45 分钟（人逐步想起遗漏）
第 2 次商品发放需求：45 分钟（人 AGAIN 逐步想起遗漏）
第 n 次商品发放需求：45 分钟（还是要想，还是那么久）

边际成本恒定，无复利效应。
知识在哪里？在人脑里，每次都要重新想起来。
```

这与我期望的"越用越快"完全相反。

### 四、转折：遇见复合工程与上下文工程

#### 4.1 复合式工程：让每一步都成为下一步的基石

在探索过程中，我接触到了"复合式工程"（Compounding Engineering）的理念。这个概念来自 Claude Code 团队与 Every 团队的实践交流，并在 Every 团队开源的 **Compound Engineering Plugin** 中得到了系统化实现——这是一个包含 27 个 Agent、19 个 Command、13 个 Skill 的完整 AI 辅助开发工具包。

##### 定义"复合式工程"

**"复合式工程"的核心目标非常明确：让每一单元的工程工作使后续工作变得更容易，而非更难。**

```
传统开发：累积技术债务 → 每个功能增加复杂性 → 代码库越来越难维护
复合工程：每个功能产出文档模式 → 创建可复用组件 → 建立减少决策疲劳的约定 → 知识在团队中复合增长
```

与传统工程中每增加一个功能都会增加系统复杂度和维护成本不同，"复合式工程"追求的是一种"复利"效应，让系统的能力随着时间推移指数级增长。

##### 核心工作流循环：Plan → Work → Review → Compound

Compound Engineering Plugin 设计了一个闭环的工作流循环：

```
Plan ──────→ Work ──────→ Review ──────→ Compound
详细规划     执行工作     质量检查       知识沉淀
   ↑                                       │
   └───────────────────────────────────────┘
            知识复合：下次规划更精准
```

* **Plan**：多代理并行研究仓库模式、最佳实践、框架文档，输出结构化计划
* **Work**：系统性执行计划，边做边测，质量内建
* **Review**：多代理并行审查（安全、性能、架构等），输出分级 Todo
* **Compound**：这是复合工程的核心——将解决的问题结构化记录，形成团队知识资产

> 完整实现参见：[Compound Engineering Plugin](https://github.com/EveryInc/compound-engineering-plugin)

##### 为什么叫"Compound"？

```
第一次解决 "N+1 query in brief generation" → Research (30 min)
文档化 → docs/solutions/performance-issues/n-plus-one-briefs.md (5 min)
下次类似问题 → Quick lookup (2 min)
知识复合 → Team gets smarter

Each unit of engineering work should make subsequent units of work easier—not harder.
```

##### 实现机制：知识复合的典型场景

实现复合工程的关键，在于建立系统化的知识沉淀机制。以下是几个典型场景：

**场景 1：Agent 重复犯同类错误**

```
触发：发现 Agent 在某类问题上反复出错
沉淀：将教训写入 AGENTS.md / CLAUDE.md / 系统提示词
效果：该类错误不再发生，无需人工提醒
```

**场景 2：某类问题需要频繁人工检查**

```
触发：Code Review 时反复指出同类问题
沉淀：创建 Lint 规则 / Pre-commit Hook / CI 检查
效果：问题在提交前自动拦截，减少人工负担
```

**场景 3：复杂流程被多次执行**

```
触发：某个多步骤操作被团队...