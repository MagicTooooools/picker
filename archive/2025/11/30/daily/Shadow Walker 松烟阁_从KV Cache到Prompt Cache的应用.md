---
title: 从KV Cache到Prompt Cache的应用
url: https://www.edony.ink/from-kv-cache-to-prompt-cache-in-ai-app/
source: Shadow Walker 松烟阁
date: 2025-11-30
fetch_date: 2025-12-01T03:39:31.141786
---

# 从KV Cache到Prompt Cache的应用

[![Shadow Walker 松烟阁](/content/images/2025/11/1573133907wings-cricut-freesvg.org.shadow.walker.for.dark.edit.svg)
![Shadow Walker 松烟阁](https://www.edony.ink/content/images/2024/01/shadow.walker.logo-5.svg)](https://www.edony.ink)

[👾 前世今生](https://www.edony.ink/about/)
[🕰️ 时光穿梭](https://wormhole.edony.ink/)
[🎙️ WalkieTalkie](https://memos.edony.ink/)
[•••](https://www.edony.ink/...)
[🚇 Travelling](https://www.travellings.cn/go.html)
[🎃 我的专栏](https://www.edony.ink/private/newsletter-columns/)
[🦶 读者足迹](https://www.edony.ink/imprints/)
[🐦 夨坕电报](https://t.me/%2ByoeQpGChKYQzYTU1)
[🚀 博客计划](https://trello.com/b/9TxzQwiI/shadow-walker)
[🐣 喋喋不休](https://www.edony.ink/twitter/)
[💯 个人清单](https://www.edony.ink/tag/list/)
[🔘 旧博归档](https://old.edony.ink/)
[📊 服务状态](https://uptime.edony.ink/status/surveillance)
[🔎 站内检索](#/search)

[👾 前世今生](https://www.edony.ink/about/)
[🕰️ 时光穿梭](https://wormhole.edony.ink/)
[🎙️ WalkieTalkie](https://memos.edony.ink/)
[•••](https://www.edony.ink/...)
[🚇 Travelling](https://www.travellings.cn/go.html)
[🎃 我的专栏](https://www.edony.ink/private/newsletter-columns/)
[🦶 读者足迹](https://www.edony.ink/imprints/)
[🐦 夨坕电报](https://t.me/%2ByoeQpGChKYQzYTU1)
[🚀 博客计划](https://trello.com/b/9TxzQwiI/shadow-walker)
[🐣 喋喋不休](https://www.edony.ink/twitter/)
[💯 个人清单](https://www.edony.ink/tag/list/)
[🔘 旧博归档](https://old.edony.ink/)
[📊 服务状态](https://uptime.edony.ink/status/surveillance)
[🔎 站内检索](#/search)
Login
Subscribe

Login
Subscribe

从KV Cache到Prompt Cache的应用

Nov 30, 2025

30 min read

[AI](/tag/ai/)

# 从KV Cache到Prompt Cache的应用

本文针对LLM的“内存墙”瓶颈，全方位解析KV Cache优化路径。从DeepSeek MLA架构演进到vLLM、SGLang系统级突破，对比Anthropic显存派与DeepSeek磁盘派的技术分化，最后结合应用代码实战，探讨Prompt Cache如何推动大模型推理迈向低成本、长窗口的持久化

![从KV Cache到Prompt Cache的应用](/content/images/size/w960/2025/11/CD154485-00A1-4240-899C-EB00F0DCA833-1.png)

Photo generated by Nano Banan Pro

## 引子

![](https://www.edony.ink/content/images/2025/11/image.png)

Screenshot from [**YouTube**](https://www.youtube.com/watch?v=EMQxQwoFSb4&ref=edony.ink)

从工程师的视角来观察，随着Scaling Law失效问题被更多的人提起，我越来越认同LLM正在逐渐进入「精打细算，收个果实的平庸时代」。Andrew Ng在他的感恩节给读者的来信中提到，AI可能存在泡沫但是一定不是在AI应用开发：

* AI 应用层： 投资不足。其潜力远超大多数人的认知。
* AI 推理基础设施： 仍需大量投资。
* AI 模型训练基础设施： 我对这一领域仍持谨慎乐观态度，但可能存在泡沫。

## 1. 大模型推理的物理瓶颈：透视KV Cache

在探讨具体的优化技术之前，必须从第一性原理出发，理解为何KV Cache会成为大模型推理的阿喀琉斯之踵。这不仅是显存容量的问题，更是显存带宽（Memory Bandwidth）与计算强度（Arithmetic Intensity）之间矛盾的体现。

### 1.1 Transformer解码的自回归特性

Transformer模型的推理过程分为两个阶段：预填充（Prefill）和解码（Decoding）。

1. 预填充阶段（Prefill）：模型并行处理输入的所有token。由于可以并行计算，这一阶段主要受限于GPU的计算能力（Compute-bound）。此时，GPU的利用率通常很高。
2. 解码阶段（Decoding）：模型逐个生成后续token。这是一个自回归（Autoregressive）过程，即生成第 $t$ 个token需要依赖于前 $t-1$ 个token的内部状态。

在标准的注意力机制（Self-Attention）中，计算公式为：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d\_k}}\right)V$$
其中，$Q$（Query）是当前时间步的查询向量，而 $K$（Key）和 $V$（Value）则包含了所有历史token的信息。为了避免在生成每一个新token时都重新计算前面所有token的 $K$ 和 $V$ 投影，系统会将这些计算好的向量存储在显存中，这就是 KV Cache。

### 1.2 显存占用的数学推导

KV Cache的显存占用量是序列长度的线性函数，且随层数、头数和隐藏层维度倍增。对于一个标准的Transformer模型，其KV Cache的大小可以通过以下公式精确计算：

$$ \text{Size}{KV} = 2 \times L{seq} \times B\_{batch} \times N\_{layers} \times H\_{heads} \times D\_{head} \times P\_{prec} $$

其中：

* $2$：代表Key和Value两个矩阵。
* $L\_{seq}$：当前的序列长度（上下文窗口大小）。
* $B\_{batch}$：并发请求的批处理大小（Batch Size）。
* $N\_{layers}$：Transformer的层数。
* $H\_{heads}$：注意力头的数量。
* $D\_{head}$：每个注意力头的维度（通常为 $D\_{model} / H\_{heads}$）。
* $P\_{prec}$：数据精度（FP16为2字节，FP32为4字节）。

案例分析：Llama-2 70B模型
假设我们使用FP16精度（2字节），序列长度为4096，Batch Size为1。

* $N\_{layers} = 80$
* $H\_{heads} = 64$ (GQA之前)
* $D\_{head} = 128$

单次请求的KV Cache大小为：

$$2 \times 4096 \times 1 \times 80 \times 64 \times 128 \times 2 \approx 10.7 \text{ GB}$$
如果我们将上下文扩展到100k tokens（如Claude或GPT-4 Turbo常见场景），单次请求的KV Cache将膨胀至 260 GB。这已经远远超过了单张NVIDIA A100 (80GB) 甚至 H100 (80GB) 的显存容量。这意味着，在长文本场景下，显存容量直接限制了Batch Size，导致GPU的算力无法被填满，推理成本急剧上升。

### 1.3 内存墙与带宽瓶颈

除了容量限制，更致命的是带宽限制。在解码阶段，每生成一个token，GPU都需要从高带宽内存（HBM）中读取整个KV Cache到片上SRAM进行计算。

* 计算量（FLOPs）：随着KV Cache的增长，计算量仅线性增长。
* 数据传输量（Bytes）：数据传输量也线性增长，但由于矩阵乘法中的一维退化为向量（Query vector），算术强度（Arithmetic Intensity，即FLOPs/Bytes比率）极低。

现代GPU（如H100）具有极高的算力（~1000 TFLOPS FP16）和极高的带宽（~3.35 TB/s）。然而，在解码阶段，由于每次矩阵向量乘法（GEMV）都需要搬运庞大的KV Cache，GPU大部分时间都在等待数据从HBM传输，导致计算单元闲置。这就是所谓的“内存受限”（Memory-bound）场景。

为了缓解这一问题，业界在过去两年中经历了从算法架构改革到系统工程优化的剧烈演变。

## 2. 注意力机制的架构演进：从MHA到MLA

![](https://www.edony.ink/content/images/2025/11/image-1.png)

为了从根本上减小KV Cache的体积，模型架构师们对Transformer的核心——注意力机制进行了多次手术。这一演进路径清晰地展示了从追求极致性能到追求效率与性能平衡的过程。

### 2.1 多头注意力（MHA）：昂贵的基准

在《Attention Is All You Need》原论文中提出的多头注意力（Multi-Head Attention, MHA）机制中，模型拥有 $H$ 个查询头（Query Heads），同时也对应拥有 $H$ 个键值头（Key/Value Heads）。

* 机制：每个Query Head都有自己独立的Key和Value投影矩阵。这意味着模型可以从 $H$ 个不同的子空间（Subspaces）捕捉信息，理论上表达能力最强。
* 代价：正如第1节中的计算所示，KV Cache的大小与头数 $H$ 成正比。对于大模型，这意味着巨大的显存开销。
* 现状：早期的BERT、GPT-2、GPT-3以及Llama-1均采用MHA。但在长上下文时代，MHA已成为不可承受之重。

### 2.2 多查询注意力（MQA）：激进的压缩

为了解决推理性价比问题，Noam Shazeer在2019年提出了多查询注意力（Multi-Query Attention, MQA）。

* 机制：无论有多少个Query Heads，整个层只保留1个 Key Head 和 1个 Value Head。所有的Query Heads共享这唯一的KV对。
* 压缩比：$H : 1$。如果模型有64个头，KV Cache的大小直接缩小64倍。
* 优势：极大地减少了显存占用和数据搬运量，使得推理速度显著提升，且能支持更大的Batch Size。
* 劣势：这种压缩过于激进，导致模型在处理复杂任务时，无法同时关注输入序列的不同方面，造成性能（Perplexity）下降和训练不稳定性。
* 应用：Google的PaLM模型和Falcon系列采用了MQA，但在开源社区并未立刻成为主流。

### 2.3 分组查询注意力（GQA）：中庸之道的胜利

在Llama-2发布时，Meta引入了分组查询注意力（Grouped-Query Attention, GQA），这一机制迅速成为当今开源大模型（如Llama-3, Mistral, Qwen）的实际标准。

* 机制：GQA是MHA和MQA的折中方案。它将Query Heads分为 $G$ 个组，每个组内的Query Heads共享一个KV Head。
* 配置：例如，Llama-2 70B使用了8个KV Heads（$G=8$），而Query Heads为64个。这意味着每8个Query Heads共享1个KV Head。
* 压缩比：$H : G$。在上述例子中，压缩比为8:1。
* 效果：研究表明，GQA在显存占用和推理速度上接近MQA，而在模型效果（Accuracy/Perplexity）上几乎等同于MHA。它成功地在帕累托前沿（Pareto Frontier）上找到了最佳平衡点。

### 2.4 多头潜在注意力（MLA）：DeepSeek的架构革命

DeepSeek-V2（及其后的V3）引入的MLA（Multi-Head Latent Attention）不仅仅是分组策略的调整，而是对KV Cache存储方式的根本性重构。这是DeepSeek能够提供极低API价格的核心技术支撑。

#### 2.4.1 低秩矩阵压缩（Low-Rank Compression）原理

传统的注意力机制直接存储投影后的 $K$ 和 $V$ 矩阵，维度为 $d\_{model} \times L$。MLA认为这些高维矩阵中存在大量的冗余信息，可以通过低秩分解来压缩。MLA不直接存储 $K$ 和 $V$，而是将输入的隐藏状态投影到一个低维的“潜在向量”（Latent Vector, $c\_{KV}$）。

* 压缩：输入向量首先经过一个下投影矩阵，变为极低维度的潜在向量（例如，压缩比可达数十倍）。
* 存储：在KV Cache中，只存储这个压缩后的潜在向量。
* 还原：在计算注意力分数时，通过一个上投影矩阵，将潜在向量实时还原为用于计算的Key和Value。

这种方法将KV Cache的显存占用从 $O(H \times d\_{head})$ 降低到了 $O(d\_{latent})$，其中 $d\_{latent}$ 远小于前者。

#### **2.4.2 解耦旋转位置编码（Decoupled RoPE）**

低秩压缩的一个巨大挑战是如何兼容旋转位置编码（RoPE）。RoPE对向量的旋转操作具有几何敏感性，直接在压缩向量上应用RoPE会破坏其位置信息，或者在还原时引入巨大误差。

DeepSeek创造性地提出了“解耦RoPE”策略：

1. 内容头（Content Head）：负责捕捉语义信息，采用上述的低秩压缩（不带RoPE）。
2. 位置头（Position Head）：一个单独的、维度很小的向量，专门用于携带RoPE位置信息。
3. 拼接（Concatenation）：在计算Attention Score时，将还原后的内容头与位置头拼接，共同参与计算。

通过这种方式，MLA既实现了极致的KV Cache压缩（仅存储压缩内容+少量位置信息），又完美保留了长上下文所需的位置感知能力。根据DeepSeek的报告，MLA使得KV Cache的大小在同等参数规模下只有GQA模型的1/5甚至更低，这使得将KV Cache放入显存之外的介质（如内存或SSD）成为可能，因为数据传输的带宽压力被大幅减轻了。

| 特性 | MHA (Llama-1) | MQA (Falcon) | GQA (Llama-3) | MLA (DeepSeek-V3) |
| --- | --- | --- | --- | --- |
| KV头数量 | 等于Query头数 ($H$) | 1 | 分组数 ($G$, 如8) | 虚拟/动态生成 |
| 显存占用 | 极高 (100%) | 极低 (~1-2%) | 中等 (~12-25%) | **极致压缩 (5-10%)** |
| 模型性能 | 基准 (高) | 有损 | 接近无损 | **无损甚至更优** |
| 推理速度 | 慢 (受限于带宽) | 极快 | 快 | 极快 |
| RoPE兼容性 | 原生支持 | 原生支持 | 原生支持 | **需解耦设计** |

## 3. 系统级显存管理与优化：从分页到流式

如果说Transformer架构决定了KV Cache的“理论最小体积”，那么系统软件则决定了如何在物理硬件上高效地“摆放”这些数据。2023年以来，以vLLM为代表的推理框架通过引入操作系统领域的经典思想，彻底改变了显存管理的范式。

### 3.1 显存碎片化与PagedAttention (vLLM)

在vLLM出现之前，主流推理框架（如FasterTransformer）采用的是静态显存分配。对于一个请求，系统必须按照其“最大可能长度”（Max Sequence Length）预先分配一块连续的显存空间。

* 内部碎片（Internal Fragmentation）：如果预分配了2048长度，但用户只生成了50个token，剩余的空间全部被浪费。
* 外部碎片（External Fragmentation）：不同请求的显存块大小不一，导致显存中出现许多无法被利用的空隙。

据统计，这种方式导致的显存浪费率高达60%-80%。

#### 3.1.1 PagedAttention的原理

![](https://www.edony.ink/content/images/2025/11/image-2.png)

vLLM团队受操作系统虚拟内存（Virtual Memory）分页机制的启发，提出了PagedAttention。

1. KV Block：将KV Cache切分为固定大小的块（Block），例如每块存储16个token的KV数据。
2. 非连续存储：这些Block在物理显存（HBM）中不需要连续存放，可以分散在任意位置。
3. 页表（Block Table）：系统维护一张映射表，记录每个请求的逻辑token顺序对应哪些物理Block。
4. 按需分配：只有当新的token生成填满当前Block时，系统才申请下一个Block。

优势：

* 零浪费：内部碎片仅存在于最后一个未填满的Block中，浪费率降至4%以下。
* 显存共享（Memory Sharing）：这是PagedAttention最强大的特性。对于这就如Python中的引用计数，如果多个请求共享相同的S...