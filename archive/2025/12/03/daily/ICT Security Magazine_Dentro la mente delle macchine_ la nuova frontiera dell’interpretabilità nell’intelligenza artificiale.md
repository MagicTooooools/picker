---
title: Dentro la mente delle macchine: la nuova frontiera dell’interpretabilità nell’intelligenza artificiale
url: https://www.ictsecuritymagazine.com/articoli/macchine-intelligenza-artificiale/
source: ICT Security Magazine
date: 2025-12-03
fetch_date: 2025-12-04T03:22:28.710878
---

# Dentro la mente delle macchine: la nuova frontiera dell’interpretabilità nell’intelligenza artificiale

[Salta al contenuto](#main)

[![ICT Security Magazine](https://www.ictsecuritymagazine.com/wp-content/uploads/2016/01/logo-ict-security.jpg)](https://www.ictsecuritymagazine.com/)

* [Home](https://www.ictsecuritymagazine.com/)
* [Articoli](https://www.ictsecuritymagazine.com/argomenti/articoli/)
* RubricheEspandi
  + [Cyber Security](https://www.ictsecuritymagazine.com/argomenti/cyber-security/)
  + [Cyber Crime](https://www.ictsecuritymagazine.com/argomenti/cyber-crime/)
  + [Cyber Risk](https://www.ictsecuritymagazine.com/argomenti/cyber-risk/)
  + [Cyber Law](https://www.ictsecuritymagazine.com/argomenti/cyber-law/)
  + [Digital Forensic](https://www.ictsecuritymagazine.com/argomenti/digital-forensic/)
  + [Digital ID Security](https://www.ictsecuritymagazine.com/argomenti/digital-id-security/)
  + [Business Continuity](https://www.ictsecuritymagazine.com/argomenti/business-continuity/)
  + [Digital Transformation](https://www.ictsecuritymagazine.com/argomenti/digital-transformation/)
  + [Cyber Warfare](https://www.ictsecuritymagazine.com/argomenti/cyber-warfare/)
  + [Ethical Hacking](https://www.ictsecuritymagazine.com/argomenti/ethical-hacking/)
  + [GDPR e Privacy](https://www.ictsecuritymagazine.com/argomenti/gdpr-e-privacy/)
  + [IoT Security](https://www.ictsecuritymagazine.com/argomenti/iot-security/)
  + [Industrial Cyber Security](https://www.ictsecuritymagazine.com/argomenti/industrial-cyber-security/)
  + [Blockchain e Criptovalute](https://www.ictsecuritymagazine.com/argomenti/blockchain-e-criptovalute/)
  + [Intelligenza Artificiale](https://www.ictsecuritymagazine.com/argomenti/intelligenza-artificiale/)
  + [Geopolitica e Cyberspazio](https://www.ictsecuritymagazine.com/argomenti/geopolitica-cyberspazio/)
  + [Interviste](https://www.ictsecuritymagazine.com/argomenti/interviste/)
* [Notizie](https://www.ictsecuritymagazine.com/argomenti/notizie/)
* [Pubblicazioni](https://www.ictsecuritymagazine.com/pubblicazioni/)
* [Cybersecurity Video](https://www.ictsecuritymagazine.com/argomenti/cybersecurity-video/)
* [Eventi](https://www.ictsecuritymagazine.com/eventi/)
* [Newsletter](https://www.ictsecuritymagazine.com/newsletter/)

[Linkedin](https://www.linkedin.com/company/ict-security-magazine/) [YouTube](https://www.youtube.com/%40ictsecuritymagazine1403) [RSS](https://www.ictsecuritymagazine.com/feed/)

[![ICT Security Magazine](https://www.ictsecuritymagazine.com/wp-content/uploads/2016/01/logo-ict-security.jpg)](https://www.ictsecuritymagazine.com/)

Attiva/disattiva menu

![Simone Scardapane al 23° Forum ICT Security presenta la decifrazione della scatola nera nei modelli di intelligenza artificiale](https://www.ictsecuritymagazine.com/wp-content/uploads/untitled-design-5_wMqvFqzp.png)

# Dentro la mente delle macchine: la nuova frontiera dell’interpretabilità nell’intelligenza artificiale

A cura di:[Simone Scardapane](#molongui-disabled-link)  Ore 3 Dicembre 20252 Dicembre 2025

*L’intervento “Decifrare la scatola nera: dall’opacità alla sicurezza” di Simone Scardapane, Professore Associato alla Sapienza Università di Roma, al 23° Forum ICT Security*

Lo sviluppo dell’intelligenza artificiale ha rivoluzionato il nostro modo di interagire con essa: oggi possiamo dialogare in linguaggio naturale con modelli generalisti come GPT o Claude, senza possedere competenze avanzate in programmazione o raccolta dati. Questa democratizzazione dell’accesso, però, convive con una forte opacità: i modelli restano spesso “scatole nere”, difficili da comprendere e da controllare, soprattutto quando generano errori o comportamenti inattesi, esponendo l’utente a numerose problematiche. È proprio da questa tensione che ha preso le mosse l’intervento di Simone Scardapane, Professore Associato alla Sapienza Università di Roma, presentato durante il [Forum ICT Security 2025](https://www.ictsecuritymagazine.com/eventi/forumictsecurity2025) con il titolo “Decifrare la scatola nera: dall’opacità alla sicurezza”.

Al centro della riflessione, il concetto di *mechanistic interpretability*: lo studio delle strutture interne dei modelli per ricostruire i principi di funzionamento delle loro componenti. Un approccio che permette di individuare i “meccanismi interni” nascosti che generano le risposte, aprendo nuove strade per rendere questi sistemi più sicuri e controllabili.

## Il paradosso dei modelli AI: capacità sovrumane ed errori banali

Negli ultimi tre o quattro anni, i Large Language Model hanno seguito una traiettoria di crescita impressionante, sia dal punto di vista computazionale sia per quanto riguarda la quantità di dati utilizzati nell’addestramento. Un aspetto particolarmente interessante di questa evoluzione riguarda le cosiddette *emerging properties*: man mano che i modelli diventano più grandi e vengono allenati su più dati, si “sbloccano” capacità che prima erano completamente assenti. Come è emerso dall’intervento, «ci sono benchmark in cui funzionano malissimo e poi, superato un certo livello di complessità, funzionano molto bene», fino a raggiungere prestazioni quasi sovrumane in diversi ambiti, dal ragionamento matematico all’analisi di immagini.

![](https://www.ictsecuritymagazine.com/wp-content/uploads/22Emergent-Capabilities22-AI-Index-Report-2024-700x394.png)

“Emergent Capabilities” (AI Index Report, 2024)

Questa dinamica genera però situazioni paradossali. Benchmark che solo un anno fa sembravano impossibili vengono oggi superati con relativa facilità dai modelli più recenti. Al contempo, gli stessi sistemi capaci di risolvere problemi complessi possono incappare in errori apparentemente banali: il professore ha citato l’esempio di un benchmark del 2025 realizzato da ricercatori dell’Università di Edimburgo, in cui i modelli venivano testati sulla capacità di leggere date e orari da orologi analogici, con risultati sorprendentemente scarsi.

![](https://www.ictsecuritymagazine.com/wp-content/uploads/22Lost-in-Time22-Saxena-et-al.-2025.png)

“Lost in Time” (Saxena et al., 2025)

«È abbastanza buffo per chi non conosce questi modelli vedere questa contrapposizione», ha commentato il relatore, «perché ci sono questi modelli che sono quasi sovraumani sotto certi punti di vista e poi fanno degli errori completamente idioti». È proprio questa incongruenza a rendere urgente la necessità di comprendere cosa accade all’interno di questi sistemi.

### Due problemi, due sfide

Da questa osservazione emergono due questioni fondamentali. La prima è di natura prettamente scientifica: come quantificare l’explainability? Come entrare effettivamente nella scatola nera per comprendere perché un modello eccelle in alcuni compiti e fallisce in altri? La seconda sfida riguarda invece la comunicazione verso gli utenti finali: una volta ottenuta questa comprensione, come renderla accessibile a chi utilizza quotidianamente questi strumenti?

L’intervento ha presentato come esempio [OLMo Trace](https://arxiv.org/abs/2504.07096), un’interfaccia sviluppata quest’anno che permette agli utenti di evidenziare parti specifiche di una risposta generata da un modello e visualizzare quali elementi dei dati di addestramento hanno contribuito a quella particolare formulazione. «Se il modello fa un’allucinazione abbastanza strana o una risposta particolarmente strana, l’utente in maniera molto semplice può capire se questo deriva dai dati da cui eravamo partiti», ha spiegato il docente.

![](https://www.ictsecuritymagazine.com/wp-content/uploads/22OLMoTrace22-Liu-et-al.-2025.png)

“OLMoTrace” (Liu et al., 2025)

Ma la vera sfida va oltre la semplice spiegazione: «Non ci interessa solo spiegare questi modelli, ma ci interessa — e questo è molto più difficile — controllarli». Non si tratta semplicemente di capire perché un modello non funziona, ma di avere «un modo preciso e chirurgico di intervenire sul comportamento di questo modello per farlo funzionare». In alcuni ambiti della computer science questo viene chiamato *recourse*: la possibilità di utilizzare l’...