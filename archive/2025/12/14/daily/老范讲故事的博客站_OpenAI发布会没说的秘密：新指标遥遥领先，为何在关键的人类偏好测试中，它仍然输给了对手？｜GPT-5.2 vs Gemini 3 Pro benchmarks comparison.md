---
title: OpenAI发布会没说的秘密：新指标遥遥领先，为何在关键的人类偏好测试中，它仍然输给了对手？｜GPT-5.2 vs Gemini 3 Pro benchmarks comparison
url: https://lukefan.com/2025/12/14/gpt-5-2-vs-gemini-3-pro-benchmarks-productivity/
source: 老范讲故事的博客站
date: 2025-12-14
fetch_date: 2025-12-15T03:30:48.914661
---

# OpenAI发布会没说的秘密：新指标遥遥领先，为何在关键的人类偏好测试中，它仍然输给了对手？｜GPT-5.2 vs Gemini 3 Pro benchmarks comparison

# [老范讲故事的博客站](https://lukefan.com)

老范的博客主站，时而会发些东西。

[![RSS](https://lukefan.com/wp-content/themes/notepad-theme/img/socialmedia/rss.png)RSS](https://lukefan.com/feed/)

* [Home](https://lukefan.com)
* [关于](https://lukefan.com/%E5%85%B3%E4%BA%8E/)

## [OpenAI发布会没说的秘密：新指标遥遥领先，为何在关键的人类偏好测试中，它仍然输给了对手？｜GPT-5.2 vs Gemini 3 Pro benchmarks comparison](https://lukefan.com/2025/12/14/gpt-5-2-vs-gemini-3-pro-benchmarks-productivity/ "OpenAI发布会没说的秘密：新指标遥遥领先，为何在关键的人类偏好测试中，它仍然输给了对手？｜GPT-5.2 vs Gemini 3 Pro benchmarks comparison")

12 月 14

Luke Fan[AIGC](https://lukefan.com/category/aigc/) [AI benchmarks](https://lukefan.com/tag/ai-benchmarks/), [AI for developers](https://lukefan.com/tag/ai-for-developers/), [AI use cases](https://lukefan.com/tag/ai-use-cases/), [AI workflow](https://lukefan.com/tag/ai-workflow/), [AIGC](https://lukefan.com/tag/aigc/), [AI推理能力](https://lukefan.com/tag/ai%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B/), [AI模型对比](https://lukefan.com/tag/ai%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94/), [AI编程](https://lukefan.com/tag/ai%E7%BC%96%E7%A8%8B/), [Claude 4.5 Opus](https://lukefan.com/tag/claude-4-5-opus/), [developer productivity](https://lukefan.com/tag/developer-productivity/), [Garlic model](https://lukefan.com/tag/garlic-model/), [GDP val](https://lukefan.com/tag/gdp-val/), [Gemini 3 Pro](https://lukefan.com/tag/gemini-3-pro/), [Google AI](https://lukefan.com/tag/google-ai/), [GPT-5.2](https://lukefan.com/tag/gpt-5-2/), [GPT-5.2 pricing](https://lukefan.com/tag/gpt-5-2-pricing/), [GPT-5.2 vs Gemini 3 Pro](https://lukefan.com/tag/gpt-5-2-vs-gemini-3-pro/), [GPT-5.2性能](https://lukefan.com/tag/gpt-5-2%E6%80%A7%E8%83%BD/), [GPT-5.2评测](https://lukefan.com/tag/gpt-5-2%E8%AF%84%E6%B5%8B/), [LM Arena](https://lukefan.com/tag/lm-arena/), [OpenAI](https://lukefan.com/tag/openai/), [Robin high model](https://lukefan.com/tag/robin-high-model/), [上下文处理](https://lukefan.com/tag/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%A4%84%E7%90%86/), [专业知识工作者](https://lukefan.com/tag/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86%E5%B7%A5%E4%BD%9C%E8%80%85/), [人工智能](https://lukefan.com/tag/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/), [代码重构](https://lukefan.com/tag/%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84/), [大语言模型](https://lukefan.com/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/), [科技评测](https://lukefan.com/tag/%E7%A7%91%E6%8A%80%E8%AF%84%E6%B5%8B/), [结构化输出](https://lukefan.com/tag/%E7%BB%93%E6%9E%84%E5%8C%96%E8%BE%93%E5%87%BA/), [长上下文](https://lukefan.com/tag/%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87/) OpenAI发布会没说的秘密：新指标遥遥领先，为何在关键的人类偏好测试中，它仍然输给了对手？｜GPT-5.2 vs Gemini 3 Pro benchmarks comparison已关闭评论

![](https://pictures.lukefan.com/gpt-5-2-vs-gemini-3-pro-benchmarks-productivity/blog_1.JPEG)

# GPT-5.2我已经用了两天了，效果到底怎么样？

大家好，欢迎收听老范讲故事的YouTube频道。

## GPT-5.2：应对Gemini的“红色警报”产品

GPT-5.2是12月11日发布的，它是在OpenAI的十周年生日这一天发布的。虽然ChatGPT-3.5，我们记得是有三年，但是在这之前，OpenAI已经苦逼了好多年了，所以这是OpenAI自己的十周年生日礼物。这个产品，应该是应对谷歌的Gemini 3 Pro的一个紧急版本，是在GPT-5的基础上进行了调整和训练得到的，更加注重长时间推理结果，结构化的输出更像系统，也就是更不像人。这个东西很多指标都遥遥领先，当然还有一个重要的特点——就是更贵。

## 两天来的个人使用感受

![](https://pictures.lukefan.com/gpt-5-2-vs-gemini-3-pro-benchmarks-productivity/blog_2.JPEG)

### 优点：输出质量显著提升

用了两天了，我自己使用的感受是什么样的？输出的结果确实要比GPT-5.1要好很多，它的输出结果更全面、更准确，而且输出的结果整个的排版格式也是非常舒服的。

### 缺点：速度极慢与应对策略

但是，**好慢好慢好慢！**重要的事情说三遍，这个东西真的是很慢。当然，也有可能是新版本刚发布，用的人很多，但确实是慢，让我把默认的thinking模式都给取消了，改成了自动模式或者直出模式。自动模式就是它根据你的问题来判断是thinking还是直出；如果是直出模式的话，就是不判断，直接给结果就完事了。当然了，我还有一个方式来应对速度，就是在GPT旁边再开个豆包，豆包是相对来说要比它快很多的。

## 恼人的上下文处理异常

![](https://pictures.lukefan.com/gpt-5-2-vs-gemini-3-pro-benchmarks-productivity/blog_3.JPEG)

另外，有一个让人使得很不爽的地方是什么？就是GPT-5.2的上下文处理经常发生异常，经常有一些很诡异的表现。什么意思？我们在一个对话里聊天，你前面问了一个问题，七嚓咔嚓给你答了一大堆，你再问下一个问题的时候，他经常把上一个问题已经答过的这些内容，又给你稀里哗啦地给你挂上。

你比如说，你前头问了123三个问题，给了你一个答案，然后你再问456三个问题，他会先总结一下前面123的三个问题的答案是什么样的，然后456这三个问题的答案是什么样的，他会给你这样的一个结果，让我自己看起来觉得有些奇怪。

所以，如果大家要去问新问题了，**最好要开启新话题**，不要在原来的对话里头一直聊下去，因为他会把前面的所有聊天内容通通放到新的对话里边去进行思考的，会影响我们答案的质量。因为我试过几次，当我突然在一个对话里头问了他一个不相干的问题的时候，他也是会把前面我们整个对话的内容再给我回顾一下，然后我后面的这个结果会严重地受到前面问答结果的影响。所以，你要问一个不相干的问题，就开个新窗口或者打开新话题。

## 图像能力：推理增强，生成不变

图片的推理跟分析确实是增强了，但是生成的部分没改。

> 本视频的全部内容都是由GPT-5.2完成内容整理的。这是最近我的一个新习惯：讲Gemini 3 Pro的内容，就完完全全用Gemini 3 Pro生成；讲DeepSeek V3.2的内容，就完完全全由Deepseek V3.2生成；讲GPT-5.1、5.2的内容，咱们就用GPT-5.1、5.2来生成。也是让大家有一个真实的体会。

## 性能揭秘：深度解读新指标GDP val

![](https://pictures.lukefan.com/gpt-5-2-vs-gemini-3-pro-benchmarks-productivity/blog_4.JPEG)

很多人说不对，GPT-5.2出来了以后，各项指标遥遥领先，都领先了Gemini 3 Pro了，按照我刚才讲的这个过程，怎么好像各有春秋的样子？

### 选择性公布的领先指标

怎么说呢？GPT-5.2的很多指标其实并没有公布，他们只公布了自己领先于Gemini 3 Pro的这些指标。但是这已经是有了巨大变化了，在以前GPT发布的所有的指标里头，它只跟自己比，它是从来不跟别人比的。所有GPT、Gemini和Anthropic的Claude相互比较的这些数据，都是社区的人，或者说媒体的人给他总结的，他们自己是从来不发布的。老大要有老大的风度，我天天去跟下边人比，这事肯定是不行的。但是这一次，他发布的这些指标都是有Gemini 3的这个数据，也有Claude 4.5 Opus的数据，它们是进行比较的。但是，OpenAI只列出了它领先的这部分，不领先的部分通通都没说。

### 全新指标：GDP val（通用开发者生产力验证）

甚至为了领先，还设置了一个OpenAI的新指标，这个指标叫GDP val（val是小写，GDP大写）。这个指标的意思是什么？叫“通用开发者生产力验证”，它是模拟真实开发工作的一个测试体系，衡量一个模型是否真的能够提升专业知识工作者，尤其是开发者的生产力。像我才算是专业知识工作者，我确实是感觉到我的生产力提升了。这个指标设计的核心是什么？

* 首先，它不关心模型会不会做选择题，也不关心模型能不能背知识点。因为什么？我们现在让模型去回答问题，都是有搜索的，或者说有这个本地知识库的，不需要模型给你编任何东西，而且我们很害怕模型给你编东西，所以这个也不是考核指标。
* 至于是不是能够命中标准答案，这件事也不重要，你设置了一个标准答案，正好答的一个字都不差，这个事也不考核。

#### 考核重点：从头到尾完成复杂任务

那他考核什么？

* 它考核的是一个复杂任务能不能从头到尾完成，输出是否可以直接用于工作。
* 中间是否出现致命的理解偏差？还有很多步的这种推理，你中间是不是理解错了？这个要去考核。
* 是否需要大量的人工返工？给我了一个结果，我还要告诉你说错了，这个地方你没理解对，那个地方我还有一个要求，这还有一个隐藏的条件你没有照顾到，这个就叫人工返工。这都属于重要的考核方向。

#### 测试过程：模拟真实开发者场景

那它测试的过程是多步骤专业任务。举个例子，阅读一份需求文件，分析约束条件（一般需求文件里都是有各种约束的嘛），然后设计解决方案，给出结构化的输出——代码、文档或者是方案，直接是让他做一个完整的任务。任务通常包括隐含条件、非显性约束、多个正确但质量不同的解法，所以为什么没有标准答案。通常会给他一个真实的开发者场景，例如：

* **重构代码：**我这个代码原来已经写好了，也能测试通过，但是它不符合高内聚、低耦合的代码规范，这种代码后续的维护、升级都比较费劲，你需要去进行重构，让一个函数变得小一点，让这个类有这种继承关系，让这个代码可以重用，这个东西叫重构。
* **补全缺失模块：**我们写好了一些代码以后，或者缺几个模块，你给我写去。我提一句话，他得给我干完，不能说在中间问你一大堆别的事情，或者给你一个半半拉拉的一个结果，这事都不行。
* **修改接口而不破坏兼容性：**经常我们会遇到什么？就是你写了一个代码，然后这个代码中间的某一个库升级了，这个库的接口就会发生变化，你需要告诉这个系统说，我现在接口变了，你现在去给我调整代码，让他重新给我兼容上去。这个也是GDP val的测试用例。
* **在限定的规则下修复问题：**这个也是我们在写代码的时候经常会遇到的一个很痛苦的事情，你说出错了，他有时候没把你要求的这个地方修改掉，还把其他很多地方给你改的乱七八糟的。现在你可以告诉他说，你就给我改这几个地方，别地儿别动，他会给你去处理。

#### 长上下文任务考核

而且会去做这种叫长上下文的任务，就是输入信息很长，包括无关信息和干扰信息都会输进来，要求模型自行判断哪些重要。考核的话就是上下文的理解能力、信息筛选能力、工作记忆的这种稳定性。但是这一块我觉得还有待提升，因为刚才我也讲了，你在这个很长的聊天过程中，如果突然问他一个很跳跃性的问题，他会搞乱掉的。这一块那你说到底是怎么更好一点？应该是允许他在工作过程中开个小差、聊个天，还是说你在工作过程中就认认真真干一个工作？反正各有取舍。

#### 评分机制与核心要求

那评分机制是什么样的？人工评审。这个任务是否完成？是否存在关键错误？是否可以直接使用？决策是否合理？是否在边界条件下崩溃？最后这个是非常非常重要的，因为我们让AI去干活，他经常会只照顾到比较普遍的这种情况，但是一些边缘的情况就会照顾不到。这个评审是要求你都要照顾到。

在这些条件里头，最重要的是什么？**一次完成**。不鼓励反复追问、人类手动修正，更看重的是一次性给出可以交付的结果。所有这些文字都是GPT-5.2直接输出的，一个字都不带差的，大家看它交付的结果还是相当可以用的。明确区分部分完成和可交付，部分完成是属于不合格的，接近正确是没有价值的。

要被严重扣分的行为是：

* 逻辑正确，但是漏掉关键约束；
* 方案可行，但是忽略明确要求。

### GDP Val的意义与得分情况

GDP Val的意义是什么？就是企业用户、AI agent的系统自动化流程、专业知识工作者，特别是开发、分析和研究的这些人，主要是给他们用的。至于你说我要娱乐一下、我要陪伴一下、我要跟他聊个天，这都不是给你用的。GDP Val不是在测模型有多聪明，我们也不需要它有多聪明，而是在测你把工作交给他，第二天能不能直接交差。这个是主要测试的目的。

得分的话，各模型表现如下：

* **GPT-5.2：**70.9分
* **GPT-5.2 Pro：**74.1分
* **Anthropic Claude Opus 4.5：**59.6分
* **Gemini 3 Pro：**53.5分
* **GPT-5.1：**38.8分
* **GPT-4：**18.6分

我觉得GDP val的指标还是非常有效的，现在我让GPT-5.2去干活的时候，拿出来的东西是基本可以直接用了，这块确实有很大提升。

## 与LM Arena榜单的对比

![](https://pictures.lukefan.com/gpt-5-2-vs-gemini-3-pro-benchmarks-productivity/blog_5.JPEG)

### GPT-5.2在LM Arena上并未领先

Gemini 3 Pro发布的时候，号称领先的LM Arena的那个指标，1,501分的那个指标，这是第一次超过1,500分，有史以来第一回。这一个指标上，GPT-5.2超过了吗？首先跟大家讲，GPT-5.2这一次发布的时候就没有公开这个指标，它只公开自己领先的指标，所有不领先的指标都没有公开，所以GPT-5.2应该是没有超过。

### 重要澄清：GPT-5.2不是Garlic

特别强调一点，GPT-5.2不是garlic。很多的文章在说garlic发布了GPT-5.2，注意，不是。大蒜模型从来没有被验证过，也没有在任何的匿名竞技场里头出现过这个garlic模型的名字，是从内部意外流传出来的。但是Garlic模型到底应对的是哪一个版本，这个现在不确定。但是所有说GPT-5.2是garlic的，都属于是产生幻觉了，甭管是人产生的幻觉，还是AI产生的幻觉，这个事本身并不重要。

### 匿名参赛：Robin与Robin high

GPT-5.2自己有没有参加LM Arena的这个竞技场？参加了，他也去做了这个评分了。但是LM Arena这个竞技场里边都是匿名的，当时Gemini 3 Pro进去的时候也是匿名的，GPT-5.2进去肯定也是匿名的。它在这个里边有两个模型：

* **Robin：**测试的评分是1,399分（Gemini 3 Pro是1,501）；
* **Robin high：**这个模型有可能是GPT-5.2 Pro，就是那个贼贵贼贵那个模型，它的测试结果是1,486分，也没有超过1,500分。

所以这一次，OpenAI就没有列这个成绩。现在普遍认为这个Robin就是GPT-5.2，Robin high应该是GPT-5.2 Pro。

### LM Arena vs GDP val：评测标准有何不同？

这个LM Arena，它叫人类偏好测试，它是由真人用户直接投票来产生的。大模型匿名参加，谁也不知道谁是谁，但是其实你说匿名，也没有那么严格，大家都能猜出来具体哪个是哪个。他的测试过程是什么？就是用户输入一个真实问题，随机抽取两个模型，不显示模型的名字，同时返回两个答案，由用户来投票。那评分的方式是每个模型上来初始分1,000分...