---
title: The Physics of Inference â€“ A Deep Dive into KV and Prompt Caching
url: https://www.edony.ink/en/the-physics-of-inference-a-deep-dive-into-kv-and-prompt-caching/
source: Shadow Walker æ¾çƒŸé˜
date: 2025-12-14
fetch_date: 2025-12-15T03:30:56.979411
---

# The Physics of Inference â€“ A Deep Dive into KV and Prompt Caching

[![Shadow Walker æ¾çƒŸé˜](/content/images/2025/11/1573133907wings-cricut-freesvg.org.shadow.walker.for.dark.edit.svg)
![Shadow Walker æ¾çƒŸé˜](https://www.edony.ink/content/images/2024/01/shadow.walker.logo-5.svg)](https://www.edony.ink)

[ğŸ‘¾ å‰ä¸–ä»Šç”Ÿ](https://www.edony.ink/about/)
[ğŸ•°ï¸ æ—¶å…‰ç©¿æ¢­](https://wormhole.edony.ink/)
[ğŸ™ï¸ WalkieTalkie](https://memos.edony.ink/)
[â€¢â€¢â€¢](https://www.edony.ink/...)
[ğŸš‡ Travelling](https://www.travellings.cn/go.html)
[ğŸƒ æˆ‘çš„ä¸“æ ](https://www.edony.ink/private/newsletter-columns/)
[ğŸ¦¶ è¯»è€…è¶³è¿¹](https://www.edony.ink/imprints/)
[ğŸ¦ å¤¨å•ç”µæŠ¥](https://t.me/%2ByoeQpGChKYQzYTU1)
[ğŸš€ åšå®¢è®¡åˆ’](https://trello.com/b/9TxzQwiI/shadow-walker)
[ğŸ£ å–‹å–‹ä¸ä¼‘](https://www.edony.ink/twitter/)
[ğŸ’¯ ä¸ªäººæ¸…å•](https://www.edony.ink/tag/list/)
[ğŸ”˜ æ—§åšå½’æ¡£](https://old.edony.ink/)
[ğŸ“Š æœåŠ¡çŠ¶æ€](https://uptime.edony.ink/status/surveillance)
[ğŸ” ç«™å†…æ£€ç´¢](#/search)

[ğŸ‘¾ å‰ä¸–ä»Šç”Ÿ](https://www.edony.ink/about/)
[ğŸ•°ï¸ æ—¶å…‰ç©¿æ¢­](https://wormhole.edony.ink/)
[ğŸ™ï¸ WalkieTalkie](https://memos.edony.ink/)
[â€¢â€¢â€¢](https://www.edony.ink/...)
[ğŸš‡ Travelling](https://www.travellings.cn/go.html)
[ğŸƒ æˆ‘çš„ä¸“æ ](https://www.edony.ink/private/newsletter-columns/)
[ğŸ¦¶ è¯»è€…è¶³è¿¹](https://www.edony.ink/imprints/)
[ğŸ¦ å¤¨å•ç”µæŠ¥](https://t.me/%2ByoeQpGChKYQzYTU1)
[ğŸš€ åšå®¢è®¡åˆ’](https://trello.com/b/9TxzQwiI/shadow-walker)
[ğŸ£ å–‹å–‹ä¸ä¼‘](https://www.edony.ink/twitter/)
[ğŸ’¯ ä¸ªäººæ¸…å•](https://www.edony.ink/tag/list/)
[ğŸ”˜ æ—§åšå½’æ¡£](https://old.edony.ink/)
[ğŸ“Š æœåŠ¡çŠ¶æ€](https://uptime.edony.ink/status/surveillance)
[ğŸ” ç«™å†…æ£€ç´¢](#/search)
Login
Subscribe

Login
Subscribe

The Physics of Inference â€“ A Deep Dive into KV and Prompt Caching

Dec 14, 2025

13 min read

# The Physics of Inference â€“ A Deep Dive into KV and Prompt Caching

This analysis provides a comprehensive dissection of KV Cache optimization. It charts the path from the architectural evolution of DeepSeekâ€™s MLA to system-level breakthroughs in vLLM and SGLang. We explore how Prompt Cache is driving inference toward a future of low-cost, persistent long contexts

![The Physics of Inference â€“ A Deep Dive into KV and Prompt Caching](/content/images/size/w960/2025/12/CD154485-00A1-4240-899C-EB00F0DCA833-1.png)

Photo generated by Nano Banan Pro

## Introduction

![](https://www.edony.ink/content/images/2025/12/image.png)

Viewed through an engineering lens, as the "Scaling Laws" face increasing scrutiny, I find myself agreeing with the growing consensus: Large Language Models (LLMs) are entering a "middle age" of calculated efficiencyâ€”a time for harvesting fruits rather than just planting forests.

In his Thanksgiving letter, Andrew Ng noted that while there may be bubbles in AI, they are certainly not in the application layer:

* **AI Application Layer:** Underinvested. The potential here far exceeds common perception.
* **AI Inference Infrastructure:** Still requires significant investment.
* **AI Training Infrastructure:** I remain cautiously optimistic, though this is where a bubble might exist.

## Context

As Generative AI transitions from experimental labs to large-scale commercial deployment, inference efficiency has become the critical variable determining economic viability. In the current landscape dominated by the Transformer architecture, the marginal cost of inference is constrained not by pure compute (FLOPs), but by the "Memory Wall."

As context windows expand from the early 4k tokens to 128k, 1M, and even 10M, managing the Key-Value (KV) Cache has emerged as the primary bottleneck for system throughput and latency.

This analysis spans from underlying physical principles to high-level application strategies. We begin by dissecting the mathematics of the KV Cache during decoding and its consumption of memory bandwidth. We then trace the architectural evolution from Multi-Head Attention (MHA) to Grouped-Query Attention (GQA), and finally to the Multi-Head Latent Attention (MLA) pioneered by DeepSeek. MLA, in particular, achieves extreme compression through the decoupling of low-rank matrix decomposition and Rotary Positional Embeddings (RoPE), laying the physical foundation for "disk-level caching."

On the system software front, we examine how vLLMâ€™s **PagedAttention** borrows paging concepts from operating systems to solve fragmentation, and how SGLangâ€™s **RadixAttention** utilizes Radix Trees for dynamic KV reuse. We also touch upon **StreamingLLM**, which exploits the "Attention Sink" phenomenon to bypass window limits for infinite streaming.

Finally, we survey the market implementation of Prompt Caching (Google, Anthropic, OpenAI, DeepSeek, Alibaba), contrasting the "High-Performance Memory" route against the "Architecture-Driven Low-Cost" route.

## 1. The Physical Bottleneck: Seeing Through the KV Cache

Before discussing optimization, we must understandâ€”from first principlesâ€”why the KV Cache is the Achilles' heel of large model inference. It is not merely a question of capacity, but a conflict between **Memory Bandwidth** and **Arithmetic Intensity**.

### 1.1 The Autoregressive Nature of Transformer Decoding

Inference in Transformers occurs in two distinct phases:

1. **Prefill Phase:** The model processes all input tokens in parallel. Because this is highly parallelizable, it is usually **Compute-bound**. GPU utilization is high.
2. **Decoding Phase:** The model generates subsequent tokens one by one. This is an **Autoregressive** process; generating the $t$-th token depends on the internal state of the previous $t-1$ tokens.

In standard Self-Attention, the calculation is:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d\_k}}\right)V$$

Here, $Q$ (Query) is the vector for the current step, while $K$ (Key) and $V$ (Value) hold information from all history tokens. To avoid recalculating the $K$ and $V$ projections for the entire history at every new step, the system stores these vectors in VRAM. This is the **KV Cache**.

### 1.2 The Math of VRAM Consumption

KV Cache size is a linear function of sequence length, multiplying with layers, heads, and dimensions. For a standard Transformer, it can be calculated as:

$$\text{Size}*{KV} = 2 \times L*{seq} \times B\_{batch} \times N\_{layers} \times H\_{heads} \times D\_{head} \times P\_{prec}$$

Where:

* $2$: Represents the two matrices, Key and Value.
* $L\_{seq}$: Current sequence length (context window).
* $B\_{batch}$: Batch size of concurrent requests.
* $N\_{layers}$: Number of layers.
* $H\_{heads}$: Number of attention heads.
* $D\_{head}$: Dimension per head.
* $P\_{prec}$: Precision (2 bytes for FP16).

**Case Study: Llama-2 70B**
Assuming FP16 precision, a sequence length of 4096, and a Batch Size of 1:

* $N\_{layers} = 80$
* $H\_{heads} = 64$
* $D\_{head} = 128$

The KV Cache for a single request is:
$$2 \times 4096 \times 1 \times 80 \times 64 \times 128 \times 2 \approx 10.7 \text{ GB}$$

If we extend the context to 100k tokens, this swells to **260 GB**. This far exceeds the capacity of a single NVIDIA A100 (80GB) or H100. Consequently, memory capacity limits Batch Size, preventing the GPU cores from being fully utilized, driving up unit costs.

### 1.3 The Memory Wall

Beyond capacity, bandwidth is the silent killer. During decoding, for every token generated, the GPU must move the *entire* KV Cache from High Bandwidth Memory (HBM) to the on-chip SRAM for calculation.

* **Compute (FLOPs):** Grows linearly.
* **Data Transfer (Bytes):** Also grows linearly.

However, because the matrix multiplication degenerates into a vector operation (Query vector), the **Arithmetic Intensity** (FLOPs/Bytes ratio) is extremely low. Even with an H100's massive bandwidth (~3.35 TB/s), the GPU spends most of its time waiting for data. This is the definition of a **Memory-bound** scenario.

## 2. Architectural Evolution: From MHA to MLA

![](https://www.edony.ink/content/images/2025/12/image-1.png)

To shrink the KV Cache, architects have performed surgery on the heart of the Transformer.

### 2.1 Multi-Head Attention (MHA): The Expensive Baseline

In the original *Attention Is All You Need*, the model has $H$ Query Heads and $H$ Key/Val...